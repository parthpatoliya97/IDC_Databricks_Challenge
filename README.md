# IDC Databricks 14 Days AI Challeng


## Day 1 of Databricks 14 Days AI Challenge

- Today wasnâ€™t just about creating an account or running Spark commands.
- It was the day I truly understood why Databricks exists. At the start, I was honestly overwhelmed.

### I had zero idea about :
- What a workspace, catalog, or volume actually means
- How data is stored and accessed inside Databricks
- What serverless compute is and why it matters
- What Spark really does behind the scenes
- Even loading data felt intimidating at first
- But step by step, things started clicking.

### ğŸ” What I learned today :
- Workspace â†’ where notebooks, workflows, and experiments live
- Catalog â†’ the governed layer that organizes data and metadata
- Volume â†’ a managed storage location for large datasets
- Serverless compute â†’ Databricks manages infra so you focus on analysis
- Spark â†’ my first time using it, and I finally understood why itâ€™s built for scale

### ğŸ› ï¸ What I worked on :
- Created a Databricks Community Edition account
- Explored Workspace, Compute, and Data Explorer
- Created my first Databricks notebook
- Ran basic PySpark commands

### ğŸ’¡ Big â€œahaâ€ moment :
- I was able to load a massive Kaggle dataset directly into a Databricks volume using just 2â€“3 lines of code â€” something I never imagined was possible without complex setup.
- Using the Kaggle API inside Databricks to download data straight into a volume felt like magic 
- This completely changed how I think about data ingestion.

### âš ï¸ What challenged me the most :
 - Understanding how compute, storage, and workspace are connected.
 - Once this clicked, Databricks started making sense as a platform, not just a tool.

------------------------------
